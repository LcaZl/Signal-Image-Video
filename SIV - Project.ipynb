{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4909fc96-c646-4a17-b808-bf2fef283450",
   "metadata": {
    "id": "4909fc96-c646-4a17-b808-bf2fef283450"
   },
   "source": [
    "# Project Signal, Image & Video\n",
    "\n",
    "## Task\n",
    "\n",
    "Given a video sequence that represents a person moving within an environment, track their movements and generate a spatio-temporal heatmap that represents the probability of occupancy of the various areas of the monitored environment over time.\n",
    "\n",
    "### Dataset\n",
    "The dataset I used for the project consists of videos from a ceiling camera in a corridor at the University of Trento, Povo 2. \n",
    "\n",
    "Given its size, the dataset is not included with the project. However, heatmaps calculated from those videos, for all the periods that the videos allowed to define, are included. These heatmaps can be viewed in Section 3 of the project.\n",
    "\n",
    "### Section 1\n",
    "This section defines the necessary modules, general functions to assist with output presentation, and file system navigation. The final cells in this part analyze the dataset to understand the available videos and set suitable time ranges.\n",
    "\n",
    "### Section 2\n",
    "This section introduces two methodologies for heatmap computation. The first couple of subsections detail the functions of the background subtraction method and a method that calculate the heatmap using YOLO people detections. The final part of this section enables the computation of the heatmaps from videos, given the time ranges. Executing this second part is not mandatory, as part 3 of the project can be used if heatmaps are already generated and placed in the appropriate folders.\n",
    "\n",
    "### Section 3\n",
    "The final section allows for the visualization of the generated heatmap and the computation of area occupancy. The process applied to the heatmap for calculating occupancy slightly differs based on the method used to generate the heatmap.\n",
    "After selecting a time range, it is possible to view all the heatmaps available for that time range, and an average heatmap will be calculated over the entire time range. This facilitates the identification of the average occupancy during that period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e9c17-6170-444a-a929-dbe851272dda",
   "metadata": {},
   "source": [
    "# Section 1 - Environment preparation and Dataset Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c18ebf-b4dc-489e-b726-67465a316d65",
   "metadata": {
    "id": "f1c18ebf-b4dc-489e-b726-67465a316d65",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Modules\n",
    "\n",
    "In this cell, the necessary modules are imported, and the drive is mounted if the project is run on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c74e4d4-3ea7-46df-8cc5-78286507bae8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6728,
     "status": "ok",
     "timestamp": 1686409440932,
     "user": {
      "displayName": "Luca Zanolo",
      "userId": "06447257521426992010"
     },
     "user_tz": -120
    },
    "id": "0c74e4d4-3ea7-46df-8cc5-78286507bae8",
    "outputId": "5c8d6fb2-2a91-4026-cacb-7603c4796c50",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting avi-r\n",
      "  Downloading avi_r-1.3.9-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from avi-r) (2.2.4)\n",
      "Collecting av>=8.0 (from avi-r)\n",
      "  Downloading av-14.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Downloading avi_r-1.3.9-py3-none-any.whl (19 kB)\n",
      "Downloading av-14.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: av, avi-r\n",
      "Successfully installed av-14.2.0 avi-r-1.3.9\n",
      "Not in Google Colab.\n",
      "Modules loaded!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Installing the avi-r module\n",
    "!pip install avi-r\n",
    "from avi_r import AVIReader\n",
    "\n",
    "# Mounting the drive in case of running on Google Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    os.chdir('/content/drive/MyDrive/ColabSIV')\n",
    "    os.listdir()\n",
    "except:\n",
    "    print('Not in Google Colab.')\n",
    "\n",
    "# Confirmation message for module loading\n",
    "print(f'Modules loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7bcd6b-d936-4b37-a62a-37134edbcd8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## File System parameter\n",
    "\n",
    "Parameters for managing the input and output during heatmap generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6debff-55fe-4eaf-be71-d1c37966d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MT1_VIDEO_PATH = 'method1_background_subtraction/videos' # Path for saving videos processed by Method 1 (background subtraction).\n",
    "\n",
    "MT1_HEATMAP_PATH = 'method1_background_subtraction/heatmaps' # Path for saving heatmaps generated by Method 1 (background subtraction).\n",
    "\n",
    "MT2_VIDEO_PATH = 'method2_yolo_detections/videos' # Path for saving videos processed by Method 2 (YOLO).\n",
    "\n",
    "MT2_HEATMAP_PATH = 'method2_yolo_detections/heatmaps' # Path for saving heatmaps generated by Method 2 (YOLO).\n",
    "\n",
    "YOLO = 'people_detection_model' # Path to the folder containing the YOLO people detection model.\n",
    "\n",
    "BACKGROUND_PATH = 'background/background.png' # Path to the image file used for background.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf3604-37e8-4c68-b16c-89c5e4e8b27d",
   "metadata": {},
   "source": [
    "## General purpose functions\n",
    "\n",
    "These functions provide general-purpose utilities for:\n",
    "\n",
    "- Visualizing data, dataframes, and series of images.\n",
    "- Retrieving videos and images from the file system.\n",
    "- Manipulating datetime and time information.\n",
    "- Analyzing the predefined dataset and available videos, with respect to defined time ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e753e39-2c8e-4ac5-8a7a-04a8db1ce024",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f4c7f52-02ec-490c-a9d7-694099e4afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot multiple images in a grid of a given dimension\n",
    "def plot_images(images, subtitle='', titles=None, images_per_row=2):\n",
    "    num_images = len(images)\n",
    "\n",
    "    # Generate as many titles as images if titles are not provided\n",
    "    if titles is None:\n",
    "        titles = [f'Img {i+1}' for i in range(num_images)]\n",
    "    elif len(titles) != num_images:\n",
    "        raise ValueError(\"Number of titles must match number of images\")\n",
    "\n",
    "    num_rows = (num_images // images_per_row) + int((num_images % images_per_row) > 0)\n",
    "    fig, axs = plt.subplots(num_rows, images_per_row, figsize=(5*images_per_row, 5*num_rows))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i in range(images_per_row * num_rows):\n",
    "        ax = axs[i]\n",
    "        if i < num_images:\n",
    "            ax.imshow(images[i])\n",
    "            ax.set_title(titles[i])\n",
    "        ax.axis('off')\n",
    "\n",
    "    fig.suptitle(subtitle, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot a single image\n",
    "def plot_image(image, title=''):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Print a title\n",
    "def print_title(title):\n",
    "    print('\\n--------------------------| ',title, ' |--------------------------\\n')\n",
    "    \n",
    "# Print a dataframe in a custom format\n",
    "def custom_print(dataframe, title = ''):\n",
    "    print(f\"{title}\\n{tabulate(dataframe, headers='keys', tablefmt='simple_grid', floatfmt='.6f', showindex=True)}\")\n",
    "\n",
    "# Generate a list of file paths from a dataframe\n",
    "def path_list(dataframe):\n",
    "    return [os.path.join(dataframe.loc[i]['Root'], dataframe.loc[i]['Filename']) for i in range(len(dataframe))]\n",
    "\n",
    "# Allow user to select an element from a given list, 0 to terminate.\n",
    "def make_choice(choice_list, title = ''):\n",
    "    clear_output()\n",
    "    print(title)\n",
    "    print(f'0 - Exit')\n",
    "    for i, key in enumerate(choice_list):\n",
    "        print(f'{i+1} - {key}')\n",
    "\n",
    "    choice = int(input('Choice : '))\n",
    "    while (choice < 1 and choice != 0 ) or choice > len(choice_list)+1:\n",
    "        choice = int(input('Choice : '))\n",
    "    \n",
    "    if choice == 0:\n",
    "        print(f'Exit.')\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f206fcf6-390d-4d16-8381-1ce68d7518ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### File System interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13e9cd5e-2721-42cf-82be-42b1e5405819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(videopath):\n",
    "    \"\"\"\n",
    "    Load a video using AVIReader.\n",
    "    \n",
    "    Args:\n",
    "        videopath (str): Path to the video file.\n",
    "    \n",
    "    Returns:\n",
    "        AVIReader: AVIReader object representing the loaded video.\n",
    "    \"\"\"\n",
    "    return AVIReader(videopath)\n",
    "\n",
    "\"\"\"\n",
    "Read images from a folder and return them as a dictionary.\n",
    "\n",
    "Args:\n",
    "    folder_path (str): Path to the folder containing the images.\n",
    "\n",
    "Returns:\n",
    "    dict: A dictionary where the keys are the image names (without file extensions) and the values\n",
    "          are the corresponding image objects.\n",
    "\"\"\"\n",
    "def read_images_from_folder(folder_path):\n",
    "    image_dict = {}\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_name = os.path.splitext(filename)[0]\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            try:\n",
    "                with Image.open(image_path) as image:\n",
    "                    image_dict[image_name] = image.copy()\n",
    "            except (IOError, OSError):\n",
    "                print(f\"Error while loading image: {filename}\")\n",
    "    \n",
    "    return image_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8705e3-5219-4478-998e-b2aacf4cc588",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Time operations\n",
    "\n",
    "Functions to assists in handling datetime/time information and divide the videos available inside the dataset based on predefined time ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58bb8a99-7153-4eb5-9902-8639986a1738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datetime(string):\n",
    "    \"\"\"\n",
    "    Extracts date, start time, and end time from a given string.\n",
    "    \n",
    "    Args:\n",
    "        string (str): The input string containing the date and time information.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the extracted date, start time, and end time.\n",
    "               Returns None if no match is found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Regular expression to extract date and time from the given string\n",
    "    datetime_regex = r'(\\d{4}-\\d{2}-\\d{2}) (\\d{2}-\\d{2}-\\d{2})~(\\d{2}-\\d{2}-\\d{2})'\n",
    "    match = re.search(datetime_regex, string)\n",
    "    \n",
    "    if match: # Extract date, start time, and end time from the matched groups\n",
    "        date = match.group(1)\n",
    "        start_time = match.group(2)\n",
    "        end_time = match.group(3)\n",
    "        \n",
    "        return date, start_time, end_time\n",
    "    else:\n",
    "        return '', '', ''\n",
    "\n",
    "\n",
    "def split_videos_by_time(videos, predefined_ranges):\n",
    "    \"\"\"\n",
    "    Splits the videos loaded with 'dataset_info' based on the given time ranges\n",
    "    \n",
    "    Args:\n",
    "        videos (pd.DataFrame): DataFrame containing video information.\n",
    "        predefined_ranges (List[Tuple[datetime.time, datetime.time]]): List of tuples\n",
    "            representing the start time and end time of each predefined range.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[Dict[str, pd.DataFrame], pd.DataFrame]: A tuple containing a dictionary\n",
    "            of split dataframes and a dataframe containing statistics for each split.\n",
    "    \"\"\"\n",
    "    dfs = {} \n",
    "    num_ranges = len(predefined_ranges)    \n",
    "    dfs_stats = pd.DataFrame(columns=['Split','Duration','St.Time','En.Time','Tot.Frames','Video.Count'])\n",
    "    \n",
    "    for i, (range_start_time, range_end_time) in enumerate(predefined_ranges):\n",
    "        # Creating an empty dataframe for each predefined time range\n",
    "        df = pd.DataFrame(columns = videos.columns)\n",
    "        \n",
    "        for k in range(len(videos)):\n",
    "            \n",
    "            video_start_time = datetime.strptime(videos.loc[k]['St.Time'], \"%H-%M-%S\")\n",
    "            video_end_time = datetime.strptime(videos.loc[k]['En.Time'], \"%H-%M-%S\")\n",
    "            \n",
    "            if (video_start_time >= range_start_time) & (video_end_time <= range_end_time):\n",
    "                # Add video tu current dataframe if it belong\n",
    "                df.loc[len(df)] = videos.loc[k]\n",
    "\n",
    "        # Creating row for general statistics aggregating current dataframe data.\n",
    "        row = [i+1, \n",
    "               timedelta(seconds=df['Frame'].sum() / df['FPS'].mean()), \n",
    "               range_start_time.time(), \n",
    "               range_end_time.time(), \n",
    "               df['Frame'].sum(),\n",
    "               len(df)]\n",
    "\n",
    "        # Update single range list of dataframe and the dataframe of statistics (dfs_stats)\n",
    "        dfs_stats.loc[i] = row\n",
    "        dfs.update({f'{range_start_time.time()}-{range_end_time.time()}' : df})\n",
    "        \n",
    "    return dfs, dfs_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2f3c8c-fdf2-4bdd-9951-4f932ce131df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Video operations\n",
    "\n",
    "All videos are read using avi-r (all available videos from the camera are in .avi format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b71db5c-716b-4b27-b294-55b220f0955a",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1686409448526,
     "user": {
      "displayName": "Luca Zanolo",
      "userId": "06447257521426992010"
     },
     "user_tz": -120
    },
    "id": "2b71db5c-716b-4b27-b294-55b220f0955a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def video_info(video, videopath):\n",
    "    \"\"\"\n",
    "    Get information about a video.\n",
    "    \n",
    "    Args:\n",
    "        video (AVIReader): AVIReader object representing the video.\n",
    "        videopath (str): Path to the video file.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list containing the video information including the filename, duration, date,\n",
    "              start time, end time, width, height, frame rate, and total frames.\n",
    "    \"\"\"\n",
    "    total_frame = video.num_frames\n",
    "    if total_frame < 0:\n",
    "        print(f\"Error: Total frame < 0 for video {videopath}\")\n",
    "        return []\n",
    "    info = [os.path.basename(videopath), \n",
    "            timedelta(seconds = total_frame / video.frame_rate),\n",
    "            *extract_datetime(videopath), \n",
    "            video.width, video.height, video.frame_rate, total_frame]\n",
    "    \n",
    "    return info\n",
    "\n",
    "\n",
    "def dataset_info(dataset_folder):\n",
    "    \"\"\"\n",
    "    Get information about the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_folder (str): Path to the dataset folder.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing a dataframe with the dataset statistics and a list of file paths\n",
    "               corresponding to the videos in the dataset.\n",
    "    \"\"\"\n",
    "    stats = pd.DataFrame(columns=['Root','Filename','Duration', 'Date', 'St.Time', 'En.Time','Width', 'Height', 'FPS', 'Frame'])\n",
    "    for root, _, files in os.walk(dataset_folder):\n",
    "        for filename in files:\n",
    "            if os.path.splitext(filename)[1].lower() in ['.mp4', '.avi', '.mkv']:\n",
    "                video = load_video(os.path.join(root, filename))\n",
    "                info = video_info(video, filename) if video else []\n",
    "                if info:\n",
    "                    stats.loc[len(stats)] = [root, *info]\n",
    "                video.close()\n",
    "            else:\n",
    "                print(f\"Skipping non-video file: {filename}\")\n",
    "    return stats, path_list(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c5a2e9-fb5f-4803-8f4f-7e0033d0fc5b",
   "metadata": {
    "id": "37c5a2e9-fb5f-4803-8f4f-7e0033d0fc5b",
    "tags": []
   },
   "source": [
    "## Dataset Analysis\n",
    "\n",
    "This section of the inspect the dataset, to understand the characteristics of the available videos and their time distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ecc87-7dd0-4805-87e2-ca7d922c9fed",
   "metadata": {
    "id": "359ecc87-7dd0-4805-87e2-ca7d922c9fed",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Inspect dataset - General\n",
    "\n",
    "Show informationa about the videos inside the dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b5ab56a-90fd-45bc-974d-5109054cfb8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2918,
     "status": "ok",
     "timestamp": 1686409453634,
     "user": {
      "displayName": "Luca Zanolo",
      "userId": "06447257521426992010"
     },
     "user_tz": -120
    },
    "id": "7b5ab56a-90fd-45bc-974d-5109054cfb8e",
    "outputId": "176a0404-8833-4ff9-ccaa-7e46d5d090b5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Available videos : 0\n",
      "\n",
      "Details:\n",
      "┌────────┬────────────┬────────────┬────────┬───────────┬───────────┬─────────┬──────────┬───────┬─────────┐\n",
      "│ Root   │ Filename   │ Duration   │ Date   │ St.Time   │ En.Time   │ Width   │ Height   │ FPS   │ Frame   │\n",
      "├────────┼────────────┼────────────┼────────┼───────────┼───────────┼─────────┼──────────┼───────┼─────────┤\n",
      "└────────┴────────────┴────────────┴────────┴───────────┴───────────┴─────────┴──────────┴───────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "# Path to folder in which videos are stored.\n",
    "DATESET_PATH = 'dataset'\n",
    "\n",
    "# Retrieve dataset information\n",
    "videos_info, paths = dataset_info(DATESET_PATH)\n",
    "\n",
    "# Show information\n",
    "print(f'\\n-> Available videos : {len(paths)}')\n",
    "custom_print(videos_info, '\\nDetails:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ace7ba3-3f04-4d53-996f-1d1c05326069",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Time ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db3e9f76-1f8b-4bea-84c2-ce27aebfb43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time ranges for analysis of the available videos.\n",
    "\n",
    "time_ranges = [\n",
    "    (pd.to_datetime(\"08:00:00\", format=\"%H:%M:%S\"), pd.to_datetime(\"08:29:59\", format=\"%H:%M:%S\")),\n",
    "    (pd.to_datetime(\"08:30:00\", format=\"%H:%M:%S\"), pd.to_datetime(\"09:00:00\", format=\"%H:%M:%S\")),\n",
    "    (pd.to_datetime(\"12:00:00\", format=\"%H:%M:%S\"), pd.to_datetime(\"12:29:59\", format=\"%H:%M:%S\")),\n",
    "    (pd.to_datetime(\"12:30:00\", format=\"%H:%M:%S\"), pd.to_datetime(\"12:59:59\", format=\"%H:%M:%S\")),\n",
    "    (pd.to_datetime(\"13:00:00\", format=\"%H:%M:%S\"), pd.to_datetime(\"14:00:00\", format=\"%H:%M:%S\")),\n",
    "    (pd.to_datetime(\"16:00:00\", format=\"%H:%M:%S\"), pd.to_datetime(\"16:59:59\", format=\"%H:%M:%S\")),\n",
    "    (pd.to_datetime(\"17:00:00\", format=\"%H:%M:%S\"), pd.to_datetime(\"17:29:59\", format=\"%H:%M:%S\")),\n",
    "    (pd.to_datetime(\"17:30:00\", format=\"%H:%M:%S\"), pd.to_datetime(\"18:00:00\", format=\"%H:%M:%S\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4823ea1-3cc1-4002-85ea-494365695090",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Inspect dataset - With time ranges\n",
    "\n",
    "Displays information about the videos in the dataset, categorizing them based on predefined time ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f620178-75c1-4179-a600-01d079bbe80f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Split videos on time ranges\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m splits, split_stats = \u001b[43msplit_videos_by_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideos_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredefined_ranges\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_ranges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Show splitted videos information\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRequested time Ranges:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36msplit_videos_by_time\u001b[39m\u001b[34m(videos, predefined_ranges)\u001b[39m\n\u001b[32m     55\u001b[39m         df.loc[\u001b[38;5;28mlen\u001b[39m(df)] = videos.loc[k]\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Creating row for general statistics aggregating current dataframe data.\u001b[39;00m\n\u001b[32m     58\u001b[39m row = [i+\u001b[32m1\u001b[39m, \n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m        \u001b[43mtimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mFrame\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mFPS\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[32m     60\u001b[39m        range_start_time.time(), \n\u001b[32m     61\u001b[39m        range_end_time.time(), \n\u001b[32m     62\u001b[39m        df[\u001b[33m'\u001b[39m\u001b[33mFrame\u001b[39m\u001b[33m'\u001b[39m].sum(),\n\u001b[32m     63\u001b[39m        \u001b[38;5;28mlen\u001b[39m(df)]\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Update single range list of dataframe and the dataframe of statistics (dfs_stats)\u001b[39;00m\n\u001b[32m     66\u001b[39m dfs_stats.loc[i] = row\n",
      "\u001b[31mValueError\u001b[39m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "# Split videos on time ranges\n",
    "splits, split_stats = split_videos_by_time(videos_info.copy(), predefined_ranges = time_ranges)\n",
    "\n",
    "# Show splitted videos information\n",
    "print(f'\\nRequested time Ranges:\\n')\n",
    "for (start,end) in time_ranges:\n",
    "    print(f'- {start.time()} - {end.time()}')\n",
    "print('\\nUsed Videos with given time ranges:', split_stats['Video.Count'].sum(),'/', len(videos_info), '\\n')\n",
    "split_stats = split_stats.rename(columns = {'St.Time':'Start time','En.Time':'End Time','Split':'Group','Tot.Frames':'Total Frames','Video.Count':'Available Videos'})\n",
    "custom_print(split_stats.replace({'St.time':'Start time','En.Time':'End Time','Split':'Video'}).set_index('Group'), '\\nAvailable videos for each time range')\n",
    "a = split_stats['Duration'].mean()\n",
    "b = split_stats['Total Frames'].mean()\n",
    "print(f'Duration: {a} - {b}')\n",
    "for period, df in splits.items():\n",
    "    print(f'\\n-----> Range: {period} ')\n",
    "    custom_print(df,'Videos:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60eaa9a-975e-439a-abbb-7b845e43d27a",
   "metadata": {},
   "source": [
    "# Section 2 - Computing heatmaps with two methodology\n",
    "\n",
    "(This part can be not executed if there already heatmaps in the appropriate folders to analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db091c-4f8c-4953-8dd0-b95d3f31a0d0",
   "metadata": {
    "id": "d5db091c-4f8c-4953-8dd0-b95d3f31a0d0"
   },
   "source": [
    "## Heatmap - Background subtraction\n",
    "\n",
    "Background subtraction technique involves isolating moving objects in a video by separating them from the background. The object is identified by subtracting the current image from a background model. The background model is the image of the video without any moving objects. It's particularly effective for videos where the main subject of interest is moving and the background remains largely static.\n",
    "\n",
    "The code uses the K-Nearest Neighbors (KNN) background subtractor model provided by OpenCV. The KNN model is a non-parametric learning algorithm that identifies moving objects based on pixel density in the image.\n",
    "\n",
    "In summary, this method computes the heatmap considering all the movement within the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b745e880-5ac9-43e8-90dc-21777a2708ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Mappings for files labelling\n",
    "\n",
    "Functions to get the string for the used parameters. These strings are used to create the names of the saved heatmaps and videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cc17a0c-728b-49d5-99d3-65a74eaac662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def morph_name(morph):\n",
    "    \"\"\"\n",
    "    This function maps the OpenCV morphological operation type to its corresponding name. \n",
    "    It takes a morphological operation type as input and returns the corresponding name. \n",
    "    If the provided morphological operation type is not mapped, it returns 'Unknown'.\n",
    "    \"\"\"\n",
    "    morph_dict = {\n",
    "        cv2.MORPH_ERODE: 'MORPH_ERODE', # Default\n",
    "        cv2.MORPH_DILATE: 'MORPH_DILATE',\n",
    "        cv2.MORPH_OPEN: 'MORPH_OPEN',\n",
    "        cv2.MORPH_CLOSE: 'MORPH_CLOSE',\n",
    "    }\n",
    "    return morph_dict.get(morph, 'Unknown')\n",
    "\n",
    "\n",
    "def morph_shape_name(morph_shape):\n",
    "    \"\"\"\n",
    "    This function maps the OpenCV morphological shape type to its corresponding name. \n",
    "    It takes a morphological shape type as input and returns the corresponding name. \n",
    "    If the provided morphological shape type is not mapped, it returns 'Unknown'.\n",
    "    \"\"\"\n",
    "    morph_shape_dict = {\n",
    "        cv2.MORPH_RECT: 'MORPH_RECT', # Default\n",
    "        cv2.MORPH_CROSS: 'MORPH_CROSS',\n",
    "        cv2.MORPH_ELLIPSE: 'MORPH_ELLIPSE',\n",
    "    }\n",
    "    return morph_shape_dict.get(morph_shape, 'Unknown')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad44cd4-21d9-4256-903f-c8ac48da682c",
   "metadata": {
    "id": "1ad44cd4-21d9-4256-903f-c8ac48da682c",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Background subtraction\n",
    "\n",
    "The heatmap_background_subtraction() function implemented in this code utilizes background subtraction method to generate a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5396c246-a6e7-4b46-b269-c3a183a985ae",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1686408219009,
     "user": {
      "displayName": "Luca Zanolo",
      "userId": "06447257521426992010"
     },
     "user_tz": -120
    },
    "id": "5396c246-a6e7-4b46-b269-c3a183a985ae"
   },
   "outputs": [],
   "source": [
    "def apply_morph(image: np.ndarray, kernel_operation=cv2.MORPH_ERODE, kernel_size: (int, int) = (3, 3), kernel_shape = cv2.MORPH_RECT, make_gaussian =  True):\n",
    "    \"\"\"\n",
    "    Apply opencv morphological operation to image and return it.\n",
    "    \n",
    "    Args:\n",
    "        image (np.ndarray): The source image to which the morphological operation is applied.\n",
    "        morph_type: Opencv morphological operation type.\n",
    "        kernel_size (int, int): Tuple of ints, representing size of kernel for morphological operation.\n",
    "        make_gaussian (bool): If true apply gaussian blur, otherwise not.\n",
    "    Returns:\n",
    "        Source image with applied chosen morphological operation.\n",
    "    \n",
    "    \"\"\"\n",
    "    kernel = cv2.getStructuringElement(kernel_shape, kernel_size) # Matrix used to modify the shape of objects in the image during the morphological operation\n",
    "    if make_gaussian:\n",
    "        image = cv2.GaussianBlur(image, (3, 3), 0) # This filter is used to reduce the noise and details of the image.\n",
    "    return cv2.morphologyEx(image, kernel_operation, kernel) # Essentially removing pixels at the edges of objects in the image\n",
    "\n",
    "\n",
    "def add_images(image1: np.ndarray, image2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Add two images together. \n",
    "    \n",
    "    Note:\n",
    "        Colors values can be bigger then 255 restriction.\n",
    "        Use np.uint64 cast so images are able to expand colors restriction above 255.\n",
    "        In order to view this image normalize it.\n",
    "    Args:\n",
    "        image1 (np.ndarray): First image to add.\n",
    "        image2 (np.ndarray): Second image to add.\n",
    "    Returns:\n",
    "        np.ndarray: Output image represeting addition of image1 and image2.\n",
    "    \"\"\"\n",
    "    return np.array(image1, dtype=np.uint64) + np.array(image2, dtype=np.uint64)\n",
    "\n",
    "\n",
    "def normalize_image(image: np.ndarray):\n",
    "    \"\"\"\n",
    "    Normalize image to 0-255 range, so it is viewed correctly.\n",
    "    \n",
    "    Args:\n",
    "        image (np.ndarray): Image for which normalization should be applied.\n",
    "    Returns:\n",
    "        np.ndarray: Normalized image.\n",
    "    \"\"\"\n",
    "    return cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "\n",
    "def apply_heatmap_colors(image: np.ndarray):\n",
    "    \"\"\"\n",
    "    Apply colors for heatmap visualisation.\n",
    "    \n",
    "    Args:\n",
    "        image (np.ndarray): Image for which heatmap colors should be applied.\n",
    "    Returns:\n",
    "        np.ndarray: Image with applied heatmap colors.\n",
    "    \"\"\"\n",
    "    return cv2.applyColorMap(image, cv2.COLORMAP_TURBO)\n",
    "\n",
    "\n",
    "def superimpose(image1: np.ndarray, image2: np.ndarray, alpha: float = 0.5):\n",
    "    \"\"\"\n",
    "    Superimpose two images with given alpha values.\n",
    "    \n",
    "    Note:\n",
    "        If alpha is closer to 1, image1 will be more visible, and if it's closer to 0, image2 will be more visible.\n",
    "    Args:\n",
    "        image1 (np.ndarray): First image to apply for superimpose operation.\n",
    "        image2 (np.ndarray): Second image to apply for superimpose operation.\n",
    "        alpha (float): Alpha of the first image. Second image gets 1 - alpha.\n",
    "            Alpha 0.5 means both images take equal part in superimpose operation. \n",
    "    Returns:\n",
    "        np.ndarray: Image after superimpose operation of image1 and image2.\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(image1, alpha, image2, 1 - alpha, 0.0)\n",
    "    \n",
    "def heatmap_background_subtraction(\n",
    "                            videopath, \n",
    "                            video_output_path,\n",
    "                            heatmap_output_path,\n",
    "                            model,\n",
    "                            alpha,  \n",
    "                            skip_frame, \n",
    "                            aggrFrame, \n",
    "                            kernel_size, \n",
    "                            kernel_operation,\n",
    "                            kernel_shape,   \n",
    "                            blur_kernel_size):\n",
    "    \"\"\"\n",
    "    This function applies the process of background subtraction to an input video. \n",
    "    The goal of this function is to compute a heatmap for a given video and save it. \n",
    "    The commented code can be uncommented if there is a need to save the video that shows the heatmap calculation process.\n",
    "    \n",
    "    Args:\n",
    "        video_input (str): Path to the input video.\n",
    "        video_output (str): Path to save the output video, if applicable.\n",
    "        image_output (str): Path to save the output heatmap image.\n",
    "        \n",
    "        model (cv2.BackgroundSubtractor): The background subtractor model used for foreground extraction. \n",
    "        In this case, `cv2.createBackgroundSubtractorKNN()` is used with `detectShadows` set to False.    \n",
    "        \n",
    "        alpha (float): The transparency value for superimposing the heatmap on the frame.       \n",
    "        skip_frame (int): The number of initial frames to skip before starting the background subtraction and heatmap generation process.         \n",
    "        aggrFrame (int): The aggregation factor for accumulating background frames. A value of 1 means that each frame is used for accumulation, resulting in a more precise heatmap.        \n",
    "        kernel_size (int, int): The size of the kernel for morphological operations. The kernel is a square-shaped matrix used for erosion and other morphological operations.        \n",
    "        kernel_operation (int): The type of morphological operation to be applied. It is set to `cv2.MORPH_ERODE`, which erodes the foreground regions.\n",
    "        kernel_shape (int): The shape of the kernel for morphological operations. It is set to `cv2.MORPH_ELLIPSE`, which represents an elliptical kernel.\n",
    "        blur_kernel_size (tuple): The size of the kernel used to blur the frame\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "        dict: A dictionary containing various image representations related to the background subtraction process and the stored heatmap.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Get the string names of the morphological operation and shape\n",
    "    kernel_op_name = morph_name(kernel_operation)\n",
    "    kernel_sh_name = morph_shape_name(kernel_shape)\n",
    "    \n",
    "    # Modify the final part of the output filename with the current heatmap parameters\n",
    "    suffix = os.path.splitext(video_output_path)[1]\n",
    "    new_suffix = f'-[A.{str(alpha)},SkFr.{str(skip_frame)},AgFr.{aggrFrame},K.{kernel_size},M.{kernel_op_name},S.{kernel_sh_name},Blur.{blur_kernel_size}]{suffix}'\n",
    "    video_output_path = video_output_path.replace(suffix, new_suffix)\n",
    "    heatmap_output_path = heatmap_output_path.replace(suffix, new_suffix)\n",
    "                \n",
    "    # Initialize background subtractor and start reading video\n",
    "    background_subtractor = model\n",
    "    video = load_video(videopath)\n",
    "                                \n",
    "    # Extract video characteristics\n",
    "    height = video.height\n",
    "    width = video.width\n",
    "    total_frames = video.num_frames\n",
    "    fps = video.frame_rate\n",
    "\n",
    "    # Initialize video writer\n",
    "    #fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    #output = cv2.VideoWriter(video_output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Initialize mask\n",
    "    accumulated_image = np.zeros((height, width), np.uint8)\n",
    "    \n",
    "    count = 0\n",
    "    with tqdm(total=total_frames) as pbar:\n",
    "        for video_frame in video.get_iter():\n",
    "            frame = video_frame.numpy()\n",
    "            \n",
    "            # Blur the frame\n",
    "            frame = cv2.blur(frame, blur_kernel_size)\n",
    "                       \n",
    "            # Apply background subtraction\n",
    "            background_filter = background_subtractor.apply(frame)\n",
    "                        \n",
    "\n",
    "            if count > skip_frame and count % aggrFrame == 0:\n",
    "            \n",
    "                # Apply morphological operation to the background filter\n",
    "                erodated_image = apply_morph(background_filter, kernel_size=kernel_size, kernel_shape=kernel_shape, kernel_operation=kernel_operation)\n",
    "                            \n",
    "\n",
    "                # Accumulate the erodated image\n",
    "                accumulated_image = add_images(accumulated_image, erodated_image)\n",
    "                            \n",
    "\n",
    "                # Normalize the accumulated image\n",
    "                normalized_image = normalize_image(accumulated_image)\n",
    "                            \n",
    "\n",
    "                # Apply heatmap colors to the normalized image\n",
    "                heatmap_image = apply_heatmap_colors(normalized_image)\n",
    "                           \n",
    "\n",
    "                # Superimpose the heatmap image on the frame\n",
    "                frames_merged = superimpose(heatmap_image, frame, alpha)\n",
    "                           \n",
    "\n",
    "                # Show current manipulated frame - Not available in google colab ------ !\n",
    "                cv2.imshow(f\"Heatmap\", frames_merged) \n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "                # Save current manipulated frame\n",
    "                #output.write(frames_merged)\n",
    "\n",
    "                # For presentation\n",
    "                if count % 100 == 0:\n",
    "                    plt.imsave(f'outputs/Bg_Sub_Processing_Steps/{count}-1.jpg', frame) \n",
    "                    plt.imsave(f'outputs/Bg_Sub_Processing_Steps/{count}-2.jpg', background_filter)\n",
    "                    plt.imsave(f'outputs/Bg_Sub_Processing_Steps/{count}-3.jpg', erodated_image)\n",
    "                    plt.imsave(f'outputs/Bg_Sub_Processing_Steps/{count}-4.jpg', accumulated_image)\n",
    "                    plt.imsave(f'outputs/Bg_Sub_Processing_Steps/{count}-5.jpg', normalized_image)\n",
    "                    plt.imsave(f'outputs/Bg_Sub_Processing_Steps/{count}-6.jpg', heatmap_image) \n",
    "                    plt.imsave(f'outputs/Bg_Sub_Processing_Steps/{count}-7.jpg', frames_merged) \n",
    "                \n",
    "            pbar.update(1)                \n",
    "            count += 1\n",
    "\n",
    "    # Save the heatmap image\n",
    "    plt.imsave(heatmap_output_path.replace('.avi','.jpg'), heatmap_image, cmap='hot')\n",
    "\n",
    "    # Release resources\n",
    "    video.close()\n",
    "    #output.release() \n",
    "                                \n",
    "    cv2.destroyAllWindows() #------ !\n",
    "                                \n",
    "    return {\n",
    "            'Last Frame': frame,\n",
    "            'Last Merged Frame': frames_merged, \n",
    "            'Heatmap': heatmap_image,\n",
    "            'Accumulated Image':accumulated_image, \n",
    "            'Normalized Image':normalized_image,\n",
    "            'Erodated Image': erodated_image\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f61ff11-6867-4e5f-8403-c92e421a5f09",
   "metadata": {
    "id": "1f61ff11-6867-4e5f-8403-c92e421a5f09"
   },
   "source": [
    "## Heatmap - YOLO\n",
    "\n",
    "In this case, the heatmap is calculated using detections from YOLO (You Only Look Once), which is a real-time object detection algorithm. This algorithm identifies various object classes, including humans, within an image or video by dividing the image into a grid and predicting several bounding boxes along with their associated class probabilities within each grid cell. Therefore, only the areas where the movement of people is detected will influence the heatmap, meaning that not every movement will contribute.\n",
    "\n",
    "I used YOLOv4-tiny to strike a good balance between performance, human detection, and model memory usage. However, any YOLO model can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc2e12f-407f-4ae2-9331-ed175eaaec48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load YOLO\n",
    "\n",
    "Load the necessary configuration, weights, and class labels required for the YOLO algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "802b3580-2ca5-420e-aaf8-2dd1dde629ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize the YOLO model\n",
    "def load_yolo_model(weights_path, cfg_path, labels_path):\n",
    "    \"\"\"\n",
    "    Initialize the YOLO model.\n",
    "    \n",
    "    Args:\n",
    "        weights_path (str): Path to the YOLO weights file.\n",
    "        cfg_path (str): Path to the YOLO configuration file.\n",
    "        labels_path (str): Path to the file containing the class labels.\n",
    "        \n",
    "    Returns:\n",
    "        net: The loaded YOLOv3 model.\n",
    "        classes (list): List of class labels.\n",
    "    \"\"\"\n",
    "    net = cv2.dnn.readNet(weights_path, cfg_path)\n",
    "    with open(labels_path, \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    return net, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BsbQteqkyZZG",
   "metadata": {
    "id": "BsbQteqkyZZG",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Heatmap with YOLO\n",
    "\n",
    "The process_video() function utilizes detections from the YOLO algorithm to compute the heatmap of a given video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2385781a-3637-418b-bca7-dd4a0c889bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for presentation\n",
    "\n",
    "def add_grid(image, grid_size):\n",
    "    \n",
    "    # Vertical lines\n",
    "    for i in range(grid_size[0], image.shape[1], grid_size[0]):\n",
    "        cv2.line(image, (i, 0), (i, image.shape[0]), (255, 0, 0), 1) \n",
    "\n",
    "    # Horizontal lines\n",
    "    for i in range(grid_size[1], image.shape[0], grid_size[1]):\n",
    "        cv2.line(image, (0, i), (image.shape[1], i), (255, 0, 0), 1)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def get_images_for_presentation(frame, heatmap_grid, grid_size, frame_count, width, height):\n",
    "    frame = add_grid(frame, grid_size)\n",
    "    plt.imsave(f'outputs/YOLO_Processing_with_Grid/{frame_count}-1.jpg', frame)\n",
    "    heatmap_grid_resized = cv2.resize(heatmap_grid, (width, height), interpolation = cv2.INTER_LINEAR)\n",
    "    heatmap_grid_resized = add_grid(heatmap_grid_resized, grid_size)\n",
    "    plt.imsave(f'outputs/YOLO_Processing_with_Grid/{frame_count}-2.jpg', heatmap_grid_resized, cmap = 'hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb017ddf-b667-4922-ad32-fcb6e20fbc7b",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1686409465416,
     "user": {
      "displayName": "Luca Zanolo",
      "userId": "06447257521426992010"
     },
     "user_tz": -120
    },
    "id": "cb017ddf-b667-4922-ad32-fcb6e20fbc7b"
   },
   "outputs": [],
   "source": [
    "def process_frame(net, classes, frame, confidence_threshold):\n",
    "    \"\"\"\n",
    "    In this function, the image is initially prepared for input to the YOLO model. Subsequently, the detection results are retrieved. \n",
    "    For the purpose of this analysis, we are exclusively interested in detections belonging to the 'person' class and any detections with a \n",
    "    confidence level falling below a certain threshold are discarded. \n",
    "    \n",
    "    For every remaining detection, the coordinates of the bounding box are extracted and returned.\n",
    "    \n",
    "    Args:\n",
    "        net (dnn_Net): The loaded YOLO model.\n",
    "        classes (list): List of class labels.\n",
    "        frame (array): The frame to process.\n",
    "        confidence_threshold (float): Confidence threshold for detections.\n",
    "        \n",
    "    Returns:\n",
    "        new_detections (list): List of new detections.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Resize the frame to 416x416 (size required by YOLO)\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255, (640, 480), (0, 0, 0), True, crop=False)\n",
    "\n",
    "    # Send frame to model\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Get detection results\n",
    "    outs = net.forward(net.getUnconnectedOutLayersNames())\n",
    "\n",
    "    # Init new detections list\n",
    "    new_detections = []\n",
    "\n",
    "    # iterate over detection results\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            # Filter only person detections with a confidence above the threshold\n",
    "            if classes[class_id] == \"person\" and confidence > confidence_threshold:\n",
    "                # Get the bounding box coordinates\n",
    "                center_x = int(detection[0] * frame.shape[1])\n",
    "                center_y = int(detection[1] * frame.shape[0])\n",
    "                w = int(detection[2] * frame.shape[1])\n",
    "                h = int(detection[3] * frame.shape[0])\n",
    "\n",
    "                new_detections.append((center_x - w // 2, center_y - h // 2, w, h))\n",
    "\n",
    "    return new_detections\n",
    "\n",
    "\n",
    "def process_video(video_path, \n",
    "                  video_output_path, \n",
    "                  heatmap_output_path, \n",
    "                  confidence_threshold, \n",
    "                  detection_frequency, \n",
    "                  grid_size,\n",
    "                  model_weights, \n",
    "                  model_cfg, \n",
    "                  labels_path):\n",
    "\n",
    "    \"\"\"\n",
    "    Process a video using predefined YOLO model and compute the heatmap using YOLO detections.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        video_output_path (str): Path for the output video file.\n",
    "        heatmap_output_path (str): Path for the output heatmap image.\n",
    "        model_weights (str): Path to the YOLOv3 weights file.\n",
    "        model_cfg (str): Path to the YOLOv3 configuration file.\n",
    "        labels_path (str): Path to the file containing the class labels.\n",
    "        confidence_threshold (float): Confidence threshold for YOLO detections.\n",
    "        detection_frequency (int): The frequency of detection in frames.\n",
    "        grid_size (tuple): The size of the grid for the heatmap.\n",
    "        \n",
    "    Returns:\n",
    "        list: List containing the normalized heatmap, resized heatmap grid and last frame processed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load YOLO model\n",
    "    net, classes = load_yolo_model(model_weights, model_cfg, labels_path)\n",
    "    \n",
    "    # Replace the final part of the output filename with the current heatmap parameters\n",
    "    suffix = os.path.splitext(video_output_path)[1]\n",
    "    new_suffix = f'-[YOLOv4-tiny,GridS.{grid_size},DectFreq.{detection_frequency},Conf.{confidence_threshold}]{suffix}'\n",
    "    video_output_path = video_output_path.replace(suffix, new_suffix)\n",
    "    heatmap_output_path = heatmap_output_path.replace(suffix, new_suffix)\n",
    "\n",
    "    # Load video\n",
    "    video = load_video(video_path)\n",
    "\n",
    "    # Get frame info\n",
    "    width = video.width\n",
    "    height = video.height\n",
    "    fps = video.frame_rate\n",
    "    total_frames = video.num_frames\n",
    "\n",
    "    # Create a video writer\n",
    "    #fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    #out = cv2.VideoWriter(video_output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Create an empty grid for the heatmap\n",
    "    heatmap_grid = np.zeros(grid_size)\n",
    "\n",
    "    frame_count = 0\n",
    "    all_coordinates = []\n",
    "\n",
    "    with tqdm(total = total_frames) as pbar:\n",
    "        for video_frame in video.get_iter():\n",
    "            frame = video_frame.numpy()\n",
    "            \n",
    "            # Perform detection only every N frames\n",
    "            if frame_count % detection_frequency == 0:\n",
    "                \n",
    "                detections = process_frame(net, classes, frame, confidence_threshold)\n",
    "                \n",
    "                for (x, y, w, h) in detections:\n",
    "                    cv2.rectangle(frame, (x, y), \n",
    "                                  (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                    # Calculate the grid cells that intersect the bounding box\n",
    "                    grid_x_start = max(int(x * grid_size[0] / width), 0)\n",
    "                    grid_x_end = min(int((x + w) * grid_size[0] / width), grid_size[0])\n",
    "                    grid_y_start = max(int(y * grid_size[1] / height), 0)\n",
    "                    grid_y_end = min(int((y + h) * grid_size[1] / height), grid_size[1])\n",
    "\n",
    "                    # Increment the count in these grid cells\n",
    "                    for grid_x in range(grid_x_start, grid_x_end):\n",
    "                        for grid_y in range(grid_y_start, grid_y_end):\n",
    "                            heatmap_grid[grid_y, grid_x] += 1\n",
    "                    \n",
    "                    all_coordinates.append((x + w // 2, y + h // 2))  # Centroid\n",
    "\n",
    "            \n",
    "                # Code for images in presentation\n",
    "                #get_images_for_presentation(frame, heatmap_grid, grid_size, frame_count, width, height)\n",
    "                #----------------------\n",
    "            \n",
    "            # Not available in google colab\n",
    "            cv2.imshow(\"Frame\", frame) #------ !\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            # Write the frame to the output video\n",
    "            #out.write(frame)\n",
    "            frame_count += 1\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Release the video and the output video\n",
    "    video.close()\n",
    "    #out.release()\n",
    "    cv2.destroyAllWindows() #------ !\n",
    "                  \n",
    "    # Check if the array is empty (all zeros) before performing the division (No detections)\n",
    "    if np.max(heatmap_grid) != 0:\n",
    "        heatmap_normalized = heatmap_grid / (np.max(heatmap_grid))\n",
    "    else:\n",
    "        heatmap_normalized = heatmap_grid\n",
    "\n",
    "    # Resize heatmap to match the video width and height\n",
    "    heatmap_grid_resized = cv2.resize(heatmap_grid, (width, height), interpolation = cv2.INTER_LINEAR)\n",
    "    plt.imsave(heatmap_output_path.replace('.avi','.jpg'), heatmap_grid_resized, cmap='hot')\n",
    "                      \n",
    "    return [heatmap_normalized, heatmap_grid_resized, frame]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d691e1d-1691-46df-8d32-c15fe450b179",
   "metadata": {},
   "source": [
    "## Generate heatmaps\n",
    "\n",
    "The code below utilizes the two previously defined methods - background subtraction and YOLO detection - to generate heatmaps from videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71b1398-9e9f-45bb-90cf-268605944c80",
   "metadata": {
    "id": "a71b1398-9e9f-45bb-90cf-268605944c80",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Generate Heatmap with Background Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ab71696-d0f7-4d51-9da2-73c94cef4f75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 914
    },
    "executionInfo": {
     "elapsed": 113950,
     "status": "ok",
     "timestamp": 1686408332954,
     "user": {
      "displayName": "Luca Zanolo",
      "userId": "06447257521426992010"
     },
     "user_tz": -120
    },
    "id": "2ab71696-d0f7-4d51-9da2-73c94cef4f75",
    "outputId": "64c1a193-cd89-410f-b1b5-4d77f573ef0b",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'splits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j, (start,end) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(time_ranges): \u001b[38;5;66;03m# Iterate on time ranges\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         df = \u001b[43msplits\u001b[49m[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart.time()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend.time()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m] \u001b[38;5;66;03m# Get the df from splits dict with key equal to considered time range\u001b[39;00m\n\u001b[32m      8\u001b[39m         rnds = []\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(video_per_range):\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m             \u001b[38;5;66;03m# Randomly select a video\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'splits' is not defined"
     ]
    }
   ],
   "source": [
    "# Number of videos to read for each time range\n",
    "video_per_range = 5\n",
    "\n",
    "try:\n",
    "    for j, (start,end) in enumerate(time_ranges): # Iterate on time ranges\n",
    "\n",
    "        df = splits[f'{start.time()}-{end.time()}'] # Get the df from splits dict with key equal to considered time range\n",
    "        rnds = []\n",
    "\n",
    "        for i in range(video_per_range):\n",
    "\n",
    "            # Randomly select a video\n",
    "            random_number = random.randint(0, len(df) - 1)\n",
    "            while random_number in rnds:\n",
    "                random_number = random.randint(0, len(df))\n",
    "            rnds.append(random_number)\n",
    "\n",
    "            video = df.loc[i] # Chosen video\n",
    "\n",
    "            # Composing paths\n",
    "            video_input = os.path.join(video['Root'], video['Filename'])\n",
    "            video_output = os.path.join(MT1_VIDEO_PATH, video['Filename'])\n",
    "            image_output = os.path.join(MT1_HEATMAP_PATH, video['Filename'])\n",
    "\n",
    "            print(f'\\n[Group {j+1}/{len(time_ranges)} - Range: {start.time()}-{end.time()}] Processing video {i+1}/{video_per_range}\\nFrom: {video_input}\\nSaving heatmap at: {image_output}\\nSaving detection video at: {video_output}')\n",
    "\n",
    "            # Calculating heatmap\n",
    "            result = heatmap_background_subtraction(\n",
    "                video_input,\n",
    "                video_output,\n",
    "                image_output,\n",
    "                model = cv2.createBackgroundSubtractorKNN(detectShadows = False),\n",
    "                alpha = 0.5,\n",
    "                skip_frame = 50,\n",
    "                aggrFrame = 1,\n",
    "                kernel_size = (4,4),\n",
    "                kernel_operation = cv2.MORPH_ERODE,\n",
    "                kernel_shape = cv2.MORPH_ELLIPSE,\n",
    "                blur_kernel_size = (3,3))\n",
    "\n",
    "            # Show info on current processed video\n",
    "            plot_images([img for key, img in result.items()],\n",
    "                        subtitle = 'Current output',\n",
    "                        titles = [key for key, img in result.items()],\n",
    "                        images_per_row = 3)\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(f'Interrupted by user')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upj6m4AHydOd",
   "metadata": {
    "id": "upj6m4AHydOd",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Generate heatmap with YOLO detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "X7vcH8L3uWyp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7vcH8L3uWyp",
    "outputId": "4e61463b-92dc-4014-987d-dd613a53b9b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing 5 videos in 08:00:00 - 08:29:59\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'splits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, (start,end) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(time_ranges): \u001b[38;5;66;03m# Select time range\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAnalyzing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_per_range\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m videos in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart.time()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend.time()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     df = \u001b[43msplits\u001b[49m[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart.time()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend.time()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m] \u001b[38;5;66;03m# Retrieve the corresponding dataframe\u001b[39;00m\n\u001b[32m      6\u001b[39m     rnds = []\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(video_per_range): \u001b[38;5;66;03m#range(len(df))\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m         \u001b[38;5;66;03m# Randomly select a video from dataframe\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'splits' is not defined"
     ]
    }
   ],
   "source": [
    "video_per_range = 5\n",
    "try:\n",
    "    for j, (start,end) in enumerate(time_ranges): # Select time range\n",
    "        print(f'\\nAnalyzing {video_per_range} videos in {start.time()} - {end.time()}')\n",
    "        df = splits[f'{start.time()}-{end.time()}'] # Retrieve the corresponding dataframe\n",
    "        rnds = []\n",
    "        for i in range(video_per_range): #range(len(df))\n",
    "\n",
    "            # Randomly select a video from dataframe\n",
    "            random_number = random.randint(0, len(df) - 1)\n",
    "            while random_number in rnds:\n",
    "                random_number = random.randint(0, len(df))\n",
    "            rnds.append(random_number)\n",
    "\n",
    "            video = df.loc[random_number] # Lock video info\n",
    "\n",
    "            video_input = os.path.join(video['Root'], video['Filename'])\n",
    "            video_output = os.path.join(MT2_VIDEO_PATH, video['Filename'])\n",
    "            image_output = os.path.join(MT2_HEATMAP_PATH, video['Filename'])\n",
    "            \n",
    "            print(f'\\n[Group {j+1}/{len(time_ranges)} - Range: {start.time()}-{end.time()}] Processing video {i+1}/{video_per_range}\\nFrom: {video_input}\\nSaving heatmap at: {image_output}\\nSaving detection video at: {video_output}')\n",
    "\n",
    "            # Generating heatmap of the extracted video\n",
    "            results = process_video('dataset/2018-12-19 17-30-00~17-34-59.avi', video_output, image_output,\n",
    "                          model_weights= YOLO + \"/yolov4-tiny.weights\",\n",
    "                          model_cfg= YOLO + \"/yolov4-tiny.cfg\",\n",
    "                          labels_path= YOLO + \"/coco.names\",\n",
    "                          confidence_threshold=0.25,\n",
    "                          detection_frequency=2,\n",
    "                          grid_size = (80,80))\n",
    "\n",
    "            # Show current output info\n",
    "            plot_images(results,\n",
    "                        subtitle = 'Current output',\n",
    "                        titles = ['Normalized Heatmap','Resized Heatmap','Last Frame'],\n",
    "                        images_per_row = 3)\n",
    "except KeyboardInterrupt:\n",
    "    print(f'Interrupted by user')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa9d77-b1ac-46aa-8a03-95a313ab8ca1",
   "metadata": {},
   "source": [
    "# Section 3 - Show generated heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad9e618-04e9-4ffc-ac34-8a734778cb91",
   "metadata": {},
   "source": [
    "## Heatmap analysis\n",
    "\n",
    "The following code display previously generated heatmaps and compute the occupancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893ced8-7bb4-4f3e-9f5a-87f541614fc4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Presentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dac16409-c559-4e36-9d79-42d477a65fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_color_range(lower_bound, upper_bound, name, size=256):\n",
    "    colors = []\n",
    "\n",
    "    r_range = range(lower_bound[0], upper_bound[0], 10)\n",
    "    g_range = range(lower_bound[1], upper_bound[1], 10)\n",
    "    b_range = range(lower_bound[2], upper_bound[2], 10)\n",
    "\n",
    "    for r in r_range:\n",
    "        for g in g_range:\n",
    "            for b in b_range:\n",
    "                colors.append((r, g, b))\n",
    "\n",
    "    width, height = len(colors), size\n",
    "    image_array = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    colors.sort(key=sum)\n",
    "    for i, color in enumerate(colors):\n",
    "        image_array[:, i] = color\n",
    "\n",
    "    # Ridimensiona l'immagine alle dimensioni desiderate\n",
    "    image_resized = cv2.resize(image_array, (640, 480))\n",
    "    return image_resized\n",
    "\n",
    "def get_masks():\n",
    "\n",
    "    # Range 1\n",
    "    lower_bound1 = np.array([0, 100, 200])\n",
    "    upper_bound1 = np.array([100, 190, 255])\n",
    "    color_range1 = generate_color_range(lower_bound1, upper_bound1, 'Mask1')\n",
    "    \n",
    "    # Range 2\n",
    "    lower_bound2 = np.array([0, 0, 120])\n",
    "    upper_bound2 = np.array([30, 30, 200])\n",
    "    color_range2 = generate_color_range(lower_bound2, upper_bound2, 'Mask2')\n",
    "\n",
    "    # Range 3\n",
    "    lower_bound3 = np.array([150, 220, 0])\n",
    "    upper_bound3 = np.array([190, 255, 50])\n",
    "    color_range3 = generate_color_range(lower_bound3, upper_bound3, 'Mask3')\n",
    "\n",
    "    # Range 4 \n",
    "    lower_bound4 = np.array([50, 220, 120])\n",
    "    upper_bound4 = np.array([100, 255, 255])\n",
    "    color_range4 = generate_color_range(lower_bound4, upper_bound4, 'Mask4')\n",
    "\n",
    "    return [color_range1,color_range2,color_range3,color_range4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b3e1a-9ab9-442d-a4c5-d5ba4c95e943",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Heatmaps manipulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4979628-ebc6-4b2f-92c3-200934be275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heatmaps(time_ranges, path):\n",
    "    \"\"\"\n",
    "    Splits the heatmaps based on the given time ranges.\n",
    "\n",
    "    Args:\n",
    "        time_ranges (List[Tuple[datetime.time, datetime.time]]): List of tuples representing\n",
    "            the start time and end time of each time range.\n",
    "        path (str): The path to the folder containing the heatmaps.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[np.ndarray]]: A dictionary mapping each time range to a list of heatmaps\n",
    "            that fall within that range.\n",
    "    \"\"\"\n",
    "    heatmaps = read_images_from_folder(path)\n",
    "\n",
    "    # Extracting the time details from the heatmaps\n",
    "    heatmap_times = {\n",
    "        name: (\n",
    "            datetime.strptime(extract_datetime(name)[1], \"%H-%M-%S\").time(),\n",
    "            datetime.strptime(extract_datetime(name)[2], \"%H-%M-%S\").time(),\n",
    "        ) \n",
    "        for name in heatmaps.keys()\n",
    "    }\n",
    "\n",
    "    # Split hatmaps\n",
    "    range_heatmaps = {\n",
    "        f'{st.time()}-{et.time()}': {\n",
    "            name : pic for name, pic in heatmaps.items()\n",
    "            if heatmap_times[name][0] >= st.time() and heatmap_times[name][1] <= et.time()\n",
    "        }\n",
    "        for st, et in time_ranges\n",
    "    }\n",
    "\n",
    "    return range_heatmaps\n",
    "    \n",
    "def highlight_hot_areas(heatmap):\n",
    "    \"\"\"\n",
    "    This function highlights the hot areas in the given heatmap image. \n",
    "    It is specifically used for the heatmap computed with the background subtraction method, applying \n",
    "    different color masks to emphasize the areas of movement.\n",
    "    \n",
    "    Args:\n",
    "        heatmap (np.ndarray): The heatmap image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The resulting image with highlighted hot areas.\n",
    "    \"\"\"   \n",
    "    # Define the color range for movement colors\n",
    "    \n",
    "    # Range 1\n",
    "    lower_bound1 = np.array([0, 100, 200])\n",
    "    upper_bound1 = np.array([100, 190, 255])\n",
    "    \n",
    "    # Range 2\n",
    "    lower_bound2 = np.array([0, 0, 120])\n",
    "    upper_bound2 = np.array([30, 30, 200])\n",
    "\n",
    "    # Range 3\n",
    "    lower_bound3 = np.array([150, 220, 0])\n",
    "    upper_bound3 = np.array([190, 255, 50])\n",
    "\n",
    "    # Range 4 \n",
    "    lower_bound4 = np.array([50, 220, 120])\n",
    "    upper_bound4 = np.array([100, 255, 255])\n",
    "\n",
    "    # Create a mask with only the pixels in bounds\n",
    "    mask1 = cv2.inRange(heatmap, lower_bound1, upper_bound1)\n",
    "    mask2 = cv2.inRange(heatmap, lower_bound2, upper_bound2)\n",
    "    mask3 = cv2.inRange(heatmap, lower_bound3, upper_bound3)\n",
    "    mask4 = cv2.inRange(heatmap, lower_bound4, upper_bound4)\n",
    "\n",
    "    # Combine masks\n",
    "    mask = cv2.bitwise_or(mask1, mask2)\n",
    "    mask = cv2.bitwise_or(mask, mask3)\n",
    "    mask = cv2.bitwise_or(mask, mask4)\n",
    "    \n",
    "    # Apply the mask to the image to obtain only the parts corresponding to hot colors\n",
    "    hot_area = cv2.bitwise_and(heatmap, heatmap, mask=mask)\n",
    "\n",
    "    # To transform all the evidenced areas into warm colors\n",
    "    hot_area = cv2.cvtColor(cv2.cvtColor(hot_area,cv2.COLOR_RGB2HSV), cv2.COLOR_BGR2RGB)\n",
    "    return hot_area, [mask1,mask2,mask3,mask4,mask]\n",
    "\n",
    "    \n",
    "def visualize_heatmap_on_background(background_path, heatmap_grid):\n",
    "    \"\"\"\n",
    "    Overlays the heatmap on a background image and returns the resulting image.\n",
    "\n",
    "    Args:\n",
    "        background_path (str): The path to the background image.\n",
    "        heatmap_grid (np.ndarray): The heatmap grid to overlay.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The resulting image with the heatmap overlay.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load background image\n",
    "    background_img = cv2.imread(background_path)\n",
    "    fig, ax = plt.subplots(figsize=(10, 10)) # Init plots\n",
    "\n",
    "    # Overlap two the two images\n",
    "    ax.imshow(background_img)\n",
    "    ax.imshow(heatmap_grid, alpha=0.5)\n",
    "\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Create a buffer to store the overlapped images\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0) # Save image in buffer\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Reopen buffer and read image\n",
    "    buf.seek(0)\n",
    "    overlaid_img = Image.open(buf)\n",
    "\n",
    "    return np.array(overlaid_img)\n",
    "    \n",
    "def show_average_heatmap(heatmap_pics, period, method, show_intermediate_results = True, threshold = 70):\n",
    "        \"\"\"\n",
    "        This function guides the creation of a heatmap over a specific period. \n",
    "        It can displays all the individual components accordingly to the show_intermediate_results parameter.\n",
    "        It show and then return the average heatmap computed for the period.\n",
    "        \n",
    "        Args:\n",
    "            heatmaps (List[np.ndarray]): List of heatmaps.\n",
    "            period (str): The time period to display the analysis for.\n",
    "            method (int): The method used to compute the input heatmaps\n",
    "            show_intermediate_results (bool): to print or not more details of the process.\n",
    "            threshold (int): The value threshold for occupancy calculation, used only with method 1. Default is '.\n",
    "    \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray] or None: A tuple containing the resulting average heatmap and the heatmap overlaid on the background. \n",
    "            Returns None if no heatmaps are available for the time period.\n",
    "        \"\"\"\n",
    "    \n",
    "        if len(heatmap_pics) > 0:\n",
    "            heatmaps = []\n",
    "            \n",
    "            if method == 0:\n",
    "                masks = get_masks()\n",
    "                index = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "                plt.imsave(f'outputs/Bg_Sub_Color_Masks/Mask1-{index}.png', masks[0])\n",
    "                plt.imsave(f'outputs/Bg_Sub_Color_Masks/Mask2-{index}.jpg', masks[1])\n",
    "                plt.imsave(f'outputs/Bg_Sub_Color_Masks/Mask3-{index}.jpg', masks[2])\n",
    "                plt.imsave(f'outputs/Bg_Sub_Color_Masks/Mask4-{index}.jpg', masks[3])\n",
    "                \n",
    "            for i, (name, heatmap_pic) in enumerate(heatmap_pics.items()):\n",
    "                            \n",
    "                pic_array = np.array(heatmap_pic)  \n",
    "\n",
    "                # Differentiate between the two methodologies\n",
    "                if method == 0: # Heatmap generated with background subtraction\n",
    "                    heatmap, intermediate_results = highlight_hot_areas(pic_array)\n",
    "                    \n",
    "\n",
    "                else: # Heatmap with YOLO\n",
    "                    heatmap = pic_array.copy()\n",
    "                    heatmap[heatmap <= threshold] = 0\n",
    "                    \n",
    "                heatmaps.append(heatmap)\n",
    "                \n",
    "                # Overlaid heatmap on background\n",
    "                heatmap_on_background = visualize_heatmap_on_background(BACKGROUND_PATH, heatmap)\n",
    "                \n",
    "                # Occupancy estimation\n",
    "                occupancy = estimate_occupancy(heatmap)\n",
    "                \n",
    "                # Show intermediate components\n",
    "                if show_intermediate_results:\n",
    "                    date, start_time, end_time = extract_datetime(name)\n",
    "                    #print_title(f'Analysis for range: {period} - Heatmap {i+1} calculated from {start_time} to {end_time}')\n",
    "\n",
    "                    if method == 0:\n",
    "                        plot_images([heatmap_pic, pic_array] + masks + intermediate_results + [heatmap, heatmap_on_background],\n",
    "                                    titles = ['Input Heatmap Picture','Heatmap Array','Color Range 1','Color Range 2','Color Range 3','Color Range 4','Mask 1','Mask 2','Mask 3','Mask 4','Generated Occupancy Mask (1,2,3,4)','Evidenced heatmap', 'Heatmap on background'], \n",
    "                                    subtitle = f'Heatmap {i+1} Period: {start_time}-{end_time} - Occupancy: {round(occupancy,2)}%', \n",
    "                                    images_per_row = 6)\n",
    "                    else:\n",
    "                        plot_images([pic_array, heatmap, heatmap_on_background], \n",
    "                                    subtitle = f'Heatmap {i+1} Period: {start_time}-{end_time} - Occupancy: {round(occupancy,2)}% - Threshold: {threshold}',\n",
    "                                    titles = ['Original Heatmap',f'Heatmap with Threshold ({threshold})','Applied on Background'],\n",
    "                                    images_per_row = 3)                       \n",
    "\n",
    "                \n",
    "            average_heatmap = (np.mean(heatmaps, axis=0)).astype(np.uint8)\n",
    "            if method == 1:\n",
    "                average_heatmap[average_heatmap <= threshold] = 0\n",
    "\n",
    "            \n",
    "            average_heatmap_on_background = visualize_heatmap_on_background(BACKGROUND_PATH, average_heatmap)\n",
    "            \n",
    "            occupancy = estimate_occupancy(average_heatmap)\n",
    "\n",
    "            # Show final result\n",
    "            #if method == 0:\n",
    "                #print_title(f'Average Heatmap')\n",
    "\n",
    "            #else:\n",
    "                #print_title(f'Average Heatmap - Threshold {threshold}')                \n",
    "            plot_images([average_heatmap, average_heatmap_on_background], \n",
    "                        subtitle=f'Average Heatmap for {period} - Average Occupancy: {round(occupancy,2)}%', \n",
    "                        titles=[f'Average Heatmap with Threshold ({threshold})', 'Average heatmap on Background'],\n",
    "                        images_per_row = 2)\n",
    "\n",
    "        else:\n",
    "            print(f'No heatmap for time range available.\\n')\n",
    "            return None\n",
    "        return average_heatmap, average_heatmap_on_background, occupancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df4ff95-671e-4f8e-bba2-c3f19eab1b8c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Occupancy evaluation functions\n",
    "\n",
    "The first of the following three functions calculates the occupancy of a heatmap for both methods. The second and third functions are used to display the average heatmap of a period calculated with the previous functions. This display is useful when used with heatmaps calculated using the second method (YOLO), as it allows the mouse click to evaluate the intensity of various areas of the heatmap, enabling adjustment of a parameter that indicates the value above which an area is considered occupied.\n",
    "\n",
    "Therefore, these functions should be executed after the heatmap for a chosen time range has been composed. At that point, the calculated heatmap will be displayed, overlaid on the background.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0be05a0f-f18e-42b1-9a37-00929baafdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_occupancy(heatmap):\n",
    "    \"\"\"\n",
    "    Estimates the occupancy percentage. It calculate the percentage of heatmap cells that are not 0.\n",
    "\n",
    "    Args:\n",
    "        heatmap (np.ndarray): The heatmap image.\n",
    "\n",
    "    Returns:\n",
    "        float: The percentage of occupied cells in the heatmap.\n",
    "    \"\"\"\n",
    "\n",
    "    active_cells = len(np.where(heatmap != 0)[0])\n",
    "    inactive_cells = len(np.where(heatmap == 0)[0])\n",
    "        \n",
    "    # Percentage of active cells\n",
    "    occupancy_percentage = (active_cells / (active_cells + inactive_cells)) * 100\n",
    "    \n",
    "    return occupancy_percentage\n",
    "\n",
    "def show_intensity(event, x, y, flags, param):\n",
    "    \"\"\"\n",
    "    Handles the mouse click event to show the average intensity of a grid cell.\n",
    "    \n",
    "    Args:\n",
    "        event (cv2.EVENT): The OpenCV mouse event.\n",
    "        x (int): The x-coordinate of the mouse event.\n",
    "        y (int): The y-coordinate of the mouse event.\n",
    "        flags (int): Any relevant flags for the event.\n",
    "        param (tuple): Additional parameters. Expects a tuple with the original image, \n",
    "                       the heatmap, and the grid size (height, width).\n",
    "    \"\"\"\n",
    "    # Mouse click event\n",
    "    if event == cv2.EVENT_LBUTTONUP:\n",
    "        image, heatmap, method = param\n",
    "\n",
    "        # Ensure pixel indices are within valid limits\n",
    "        x = min(x, image.shape[1] - 1)\n",
    "        y = min(y, image.shape[0] - 1)\n",
    "\n",
    "        # Calculate intensity at the pixel\n",
    "        pixel_intensity = np.mean(heatmap[y, x])\n",
    "        if pixel_intensity < pixel_intensity and method == 1:\n",
    "            pixel_intensity = 0\n",
    "        print(f\"Intensity at pixel ({x}, {y}): {pixel_intensity}\")\n",
    "\n",
    "def show_intensity_on_image(image, heatmap, method):\n",
    "    \"\"\"\n",
    "    Visualizes the grid overlay on an image and sets up the mouse callback to show \n",
    "    the intensity when a pixel is clicked. The intensity is retrieved from the heatmap.\n",
    "    The image showed is the average heatmap overlapped on background.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The original image.\n",
    "        heatmap (np.ndarray): The heatmap.\n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)\n",
    "    heatmap = np.array(heatmap)\n",
    "\n",
    "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
    "    \n",
    "    # Register the mouse callback function\n",
    "    cv2.namedWindow(\"Inspect itensity\")\n",
    "    cv2.setMouseCallback(\"Inspect itensity\", show_intensity, (image, heatmap, method))\n",
    "\n",
    "    # Show the image\n",
    "    cv2.imshow(\"Inspect itensity\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7769c52-4ce0-44e1-991a-708da091c4a2",
   "metadata": {},
   "source": [
    "## Heatmap Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e2b6f-18d3-4c04-ba68-bc78ece7f04c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Visualize heatmaps generated with background subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c6b75f1-b4f8-44d7-b2ea-024b795265b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select time range:\n",
      "\n",
      "0 - Exit\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m heatmaps_splitted = split_heatmaps(time_ranges, MT1_HEATMAP_PATH)\n\u001b[32m      3\u001b[39m available_choices = \u001b[38;5;28mlist\u001b[39m([key \u001b[38;5;28;01mfor\u001b[39;00m key, \u001b[38;5;28mlist\u001b[39m \u001b[38;5;129;01min\u001b[39;00m heatmaps_splitted.items() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m) > \u001b[32m0\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m choice = \u001b[43mmake_choice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mavailable_choices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSelect time range:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m choice != \u001b[32m0\u001b[39m:\n\u001b[32m      8\u001b[39m     index = available_choices[choice - \u001b[32m1\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mmake_choice\u001b[39m\u001b[34m(choice_list, title)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(choice_list):\n\u001b[32m     53\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m choice = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mChoice : \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m (choice < \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m choice != \u001b[32m0\u001b[39m ) \u001b[38;5;129;01mor\u001b[39;00m choice > \u001b[38;5;28mlen\u001b[39m(choice_list)+\u001b[32m1\u001b[39m:\n\u001b[32m     57\u001b[39m     choice = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mChoice : \u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "# Bring in memory available heatmaps calculated with background subtraction\n",
    "heatmaps_splitted = split_heatmaps(time_ranges, MT1_HEATMAP_PATH)\n",
    "available_choices = list([key for key, list in heatmaps_splitted.items() if len(list) > 0])\n",
    "\n",
    "choice = make_choice(available_choices, title = f'Select time range:\\n')\n",
    "\n",
    "if choice != 0:\n",
    "    index = available_choices[choice - 1]\n",
    "    heatmap, heatmap_on_background, occupancy = show_average_heatmap(heatmap_pics = heatmaps_splitted[index], \n",
    "                                                       period = index, \n",
    "                                                       method = 0, \n",
    "                                                       show_intermediate_results = True)\n",
    "    index = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    plt.imsave(f'outputs/Bg_Sub_Average_Heatmap/Heatmap{index}.jpg', heatmap)\n",
    "    plt.imsave(f'outputs/Bg_Sub_Average_Heatmap/HeatmapOnBackground-{index}.jpg', heatmap_on_background)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52dd18e-6159-42fc-9bc9-0fd6a741e00b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dinamically detect heatmap intensity\n",
    "\n",
    "Analyze intensity on previously generated heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f910821a-f077-44b0-9f46-7dcace8d7eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_intensity_on_image(heatmap_on_background, heatmap, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df8f5bf-4054-4729-a99a-83d0aa917da1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Visualise heatmaps generated with YOLO detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfcdd180-0502-4868-892d-b63d26ecd870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select time range:\n",
      "\n",
      "0 - Exit\n",
      "Exit.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Note for M2_HEATMAP_THRESHOLD\n",
    "\n",
    "Occupancy level for occupancy percentage calculation.\n",
    "By increasing this value, only areas with a value above the threshold will contribute to the heatmap calculation.\n",
    "Can be used to smooth the heatmap intensity and narrow the area of interest to the hottest ones.\n",
    "For example, with 0 it use all shades of the heatmap in calculating masks and occupancy.\n",
    "\"\"\"\n",
    "M2_HEATMAP_THRESHOLD = 40\n",
    "\n",
    "heatmaps_splitted = split_heatmaps(time_ranges, MT2_HEATMAP_PATH) # Bring in memory available heatmap calculated with background subtraction\n",
    "available_choices = list([key for key, list in heatmaps_splitted.items() if len(list) > 0])\n",
    "\n",
    "choice = make_choice(available_choices, title = f'Select time range:\\n')\n",
    "\n",
    "if choice != 0:\n",
    "    index = available_choices[choice - 1]\n",
    "    heatmap, heatmap_on_background, occupancy = show_average_heatmap(heatmap_pics = heatmaps_splitted[index], \n",
    "                                                           period = index, \n",
    "                                                           method = 1, \n",
    "                                                           show_intermediate_results = False,\n",
    "                                                           threshold = M2_HEATMAP_THRESHOLD)\n",
    "    \n",
    "    index = index = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    plt.imsave(f'outputs/YOLO_Average_Heatmap/HeatmapWithThreshold-{index}.jpg', heatmap)\n",
    "    plt.imsave(f'outputs/YOLO_Average_Heatmap/HeatmapOnBackground-{index}.jpg', heatmap_on_background)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b768d4b-2db3-4199-83f1-bee303ce467a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dinamically detect heatmap intensity\n",
    "\n",
    "Analyze intensity on previously generated heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90833cd0-af85-4269-a438-aff0cca7d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_intensity_on_image(heatmap_on_background, heatmap, 1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "YjXSuFoEKFDg",
    "554c1662-0b7f-4925-8714-a5cb71a608ec"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
